\documentclass{article}
\usepackage{ctex}

\title{An introduction to entropy, cross entropy and KL divergence in machine learning}
\author{https://adventuresinmachinelearning.com/cross-entropy-kl-divergence/}
\begin{document}
\maketitle   % 生成标题
If you’ve been involved with neural networks and have beeen using them for classification, you almost certainly will have used a cross entropy loss function. However, have you really understood what cross-entropy means? Do you know what entropy means, in the context of machine learning? If not, then this post is for you. In this introduction, I’ll carefully unpack the concepts and mathematics behind entropy, cross entropy and a related concept, KL divergence, to give you a better foundational understanding of these important ideas. For starters, let’s look at the concept of entropy.

\end{document}
